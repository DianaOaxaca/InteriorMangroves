{
  "hash": "1c0c73185d497a4d493c2a1ba971e902",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"01.Preprocessing\"\n---\n\n\n\n\n::: callout-tip\n## Workspace\n\n`Directory` structure and content information\n\n`data/` Directory with raw sequences and symbolic links of sequences.\n\n`src/` Scripts directory to perform each analysis\n\n`results/` Contains results of each analysis.\n\n`outs/` Contains the output reports of each analysis.\n:::\n\n## 01. Remove adapters\n\n``` markdown\n#Remove primers\n\nmkdir -p results/02.cutadapt\nout=\"results/02.cutadapt\"\nFASTQ=$(ls data/*.gz | sed 's/_.*//' | sed 's/data\\///' | sort -u)\n\ndate\n\nfor FILE in ${FASTQ[@]}; do\n    echo -e \"Run cutadapt to $FILE sample\"\n        CUTADAPT='cutadapt -m 200 --pair-filter any --no-indels -g CCTACGGGNGGCWGCAG -G GACTACHVGGGTATCTAATCC -Z -j 80 -o '$out'/'$FILE'_1.fastq.gz -p '$out'/'$FILE'_2.fastq.gz data/'$FILE'_R1.fastq.gz  data/'$FILE'_R2.fastq.gz'\n        echo -e $CUTADAPT \"\\n\"\n        $CUTADAPT\ndone\n```\n\n## 02. Get ASVs with DADA2\n\n``` bash\n# Get path info\ngetwd()\n\n# Load packages\nlibrary(Rcpp)\nlibrary(dada2)\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n##### 01. Prepare files -------------------------------------------------------\n\n#Load trim fastq files and list fastq_path content\nfastq_path <- \"/axolote/diana/manglares/v0.2/results/02.cutadapt\"\nlist.files(fastq_path) \n\n#Sort file names\nFs <- sort(list.files(fastq_path, pattern=\"_1.fastq\"))\nRs <- sort(list.files(fastq_path, pattern=\"_2.fastq\"))\n\n# Extract sample names\nsampleNames <- sapply(strsplit(Fs, \"_1\"), `[`, 1)\nsampleNames\n\n# Add complete path to remove ambiguities errors\nFs <- file.path(fastq_path, Fs)\nRs <- file.path(fastq_path, Rs)\n\n##### 02. Check Quality --------------------------------------------------------\n\n# Quality check plot with only the first fastq file\nQC_F1_15 <- plotQualityProfile(Fs[1:15], aggregate = TRUE)\nQC_R1_15 <- plotQualityProfile(Rs[1:15], aggregate = TRUE)\nQCsFR1_15 <- grid.arrange(QC_F1_15, QC_R1_15, nrow = 1)\n\n#save in png format\nggsave(\"results/plots/01.QualityProfile_1-15.png\", QCsFR1_15, width = 7, height = 3)\n\n#save in pdf format\nQCsFR1_15 <- grid.arrange(QC_F1_15, QC_R1_15, nrow = 1)\nggsave(\"results/plots/01.QualityProfile_1-15.pdf\", QCsFR1_15, width = 7, height = 3)\n\n##### 03.Quality control -------------------------------------------------------\n\n# Create directory for clean reads\nfilt_path <- file.path(\"results/03.Dada2\" , \"01.filter_reads\") \nif(!file_test(\"-d\", filt_path)) \n  dir.create(filt_path)\nfiltFs <- file.path(filt_path, paste0(sampleNames, \"_F_filt.fastq.gz\"))\nfiltRs <- file.path(filt_path, paste0(sampleNames, \"_R_filt.fastq.gz\"))\n\n# Filter versions\n\n# V1 \nout1 <- filterAndTrim(Fs, filtFs, Rs, filtRs,\n                      truncLen=c(250,200),\n                      maxN=0, maxEE=c(5,5), truncQ=2, rm.phix=TRUE,\n                      compress=TRUE, multithread=TRUE) \nhead(out1)\n\n#V2 extra permissive\nout2 <- filterAndTrim(Fs, filtFs, Rs, filtRs,\n                      maxN=0, maxEE=c(5,5), truncQ=2, rm.phix=TRUE,\n                      compress=TRUE, multithread=TRUE) \nhead(out2)\n\n#V3\nout3 <- filterAndTrim(Fs, filtFs, Rs, filtRs,\n                      truncLen=c(280,200),\n                      maxN=0, maxEE=c(5,5), truncQ=2, rm.phix=TRUE,\n                      compress=TRUE, multithread=TRUE) \nhead(out3)\n\n##v4\nout4 <- filterAndTrim(Fs, filtFs, Rs, filtRs,\n                      truncLen=c(0,200),\n                      maxN=0, maxEE=c(5,5), truncQ=2, rm.phix=TRUE,\n                      compress=TRUE, multithread=TRUE) \nhead(out4)\n\n##v5\nout5 <- filterAndTrim(Fs, filtFs, Rs, filtRs,\n                      truncLen=c(260,200),\n                      maxN=0, maxEE=c(5,5), truncQ=2, rm.phix=TRUE,\n                      compress=TRUE, multithread=TRUE)\nhead(out5)\n\n## compare trunc versions\nv1 <- as.data.frame(out1)\nv2 <- as.data.frame(out2)\nv3 <- as.data.frame(out3)\nv4 <- as.data.frame(out4)\nv5 <- as.data.frame(out5)\n\n\n# Percentage function\ncalculate_percentage <- function(df, group_name) {\n  df$percentage <- (df$reads.out / df$reads.in) * 100\n  df$version <- group_name\n  return(df)\n}\n\n# Get percentage\nout1_with_percentage <- calculate_percentage(v1, 'v1:250-200')\nout2_with_percentage <- calculate_percentage(v2, 'v2:0-0')\nout3_with_percentage <- calculate_percentage(v3, 'v3:280-200')\nout4_with_percentage <- calculate_percentage(v4, 'v4:0-200')\nout5_with_percentage <- calculate_percentage(v5, 'v5:260-200')\n\n# Combine percentage versions\ncombined_data <- rbind(out1_with_percentage, out2_with_percentage, \n                       out3_with_percentage, out4_with_percentage,\n                       out5_with_percentage)\n\n# Compare plot\nboxplot_versions <- ggplot(combined_data, aes(x = version, y = percentage, \n                    fill = version)) + geom_boxplot() + theme_bw() +\n  labs(x = \"Filter version\", y = \"Percentage of reads after filter\") +\n  scale_fill_brewer(palette = \"Set2\")\n\nboxplot_versions\n\n#save plot as png\nggsave(\"results/plots/02.boxplot_trunc_versions.png\", boxplot_versions, width = 6)\n\n#save plot as pdf\nggsave(\"results/plots/02.boxplot_trunc_versions.pdf\", boxplot_versions, width = 6)\n\n#Save info of final version\n#We chose v5 \nwrite.table(out5_with_percentage, file=\"results/03.Dada2/Dada_clean_reads.tsv\", quote=F, sep=\"\\t\",col.names=NA) # Table with the totals before and after cleaning\n\n##### 04.Error Model -----------------------------------------------------------\n\n#De-replicate to reduce redundance \n\nderepFs <- derepFastq(filtFs, verbose=TRUE)\nderepRs <- derepFastq(filtRs, verbose=TRUE)\n\n# Add names to de-rep object\nnames(derepFs) <- sampleNames\nnames(derepRs) <- sampleNames\n\n#Generate Error model IMPORTANT\nerrF <- learnErrors(derepFs, multithread=TRUE, verbose = TRUE)\nerrR <- learnErrors(derepRs, multithread=TRUE, verbose=TRUE)\n\nsave.image(file = \"src/Dada2.RData\") # Save point to stop for now\n\n##### 0.5 Get ASVs -------------------------------------------------------------\n# ASVs inference\ndadaFs <- dada(derepFs, err=errF, multithread=TRUE, pool = \"pseudo\", verbose=TRUE)\ndadaRs <- dada(derepRs, err=errR, multithread=TRUE, pool = \"pseudo\", verbose = TRUE)\n\nsave.image(file = \"src/Dada2.RData\") # Save point to stop for now\n\n# Merge pairs\nmergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, minOverlap = 8, verbose=TRUE)\n\n# Create ASVs table \nseqtabAll <- makeSequenceTable(mergers)\ntable(nchar(getSequences(seqtabAll)))\n\n# Remove chimeras\nseqtab_nochim <- removeBimeraDenovo(seqtabAll, method=\"consensus\", multithread=TRUE, verbose=TRUE)\n\ndim(seqtab_nochim)\nsum(seqtab_nochim)/sum(seqtabAll)\n\n##### 0.6 info -----------------------------------------------------------------\n\n# create a new table with each ASV number and its representative sequence\nPE.table_tsv_output <- seqtab_nochim\nPE.table_tsv_output[PE.table_tsv_output==1]=0 # Don't consider those values that have a single observation per sample, make them 0 (sample singletons)\nPE.table_tsv_output <- PE.table_tsv_output[,colSums(PE.table_tsv_output)>1] # filter singleton ASVs across the table\n\n# Export sequences as in fasta format\nuniquesToFasta(PE.table_tsv_output, fout=\"results/03.Dada2/ASVs.fasta\", ids=paste(\"ASV_\",1:ncol(PE.table_tsv_output), sep=\"\"))\nnochim=PE.table_tsv_output\nwrite.table(cbind(\"ASVs\"=1:nrow(t(PE.table_tsv_output)),\"rep_seq\"=rownames(t(PE.table_tsv_output))), file=\"results/03.Dada2/ASV_to_seqs-nochim.tsv\", quote=F, sep=\"\\t\",row.names=FALSE)\n\n# replace the rep_seq with an incremental ASV number\nPE.table_tsv_output <- t(PE.table_tsv_output)\nrownames(PE.table_tsv_output) <- paste0(\"ASV_\",1:nrow(PE.table_tsv_output))\n\n# and print the output ASV table\nwrite.table(PE.table_tsv_output, file=\"results/03.Dada2/ASV_to_seqs-nochim.tsv\", quote=F, sep=\"\\t\",col.names=NA)\n\n# evaluate the total table dimensions\ndim(nochim)\n\ntable(nchar(getSequences(nochim))) \n\n###Track reads lost per step ###\n\n# By using this, we can create a function to automate this for all samples in a set:\ngetN <- function(x) sum(getUniques(x)) # Where getUniques gets non-repeated sequences from a dada2 object or merger object (joined reads)\ntrack <- cbind(out1, sapply(derepFs, getN), sapply(dadaFs, getN), sapply(dadaRs, getN), rowSums(seqtabAll), rowSums(nochim))\ncolnames(track) <- c(\"Raw\", \"Qual_filter\", \"Derep\", \"ASVs R1\", \"ASVs R2\", \"Merged\", \"nonchim\")\nrownames(track) <- sampleNames\nwrite.table(track, \"results/03.Dada2/Seqs_lost_in_ASVs_processing.tsv\", col.names=NA, sep=\"\\t\")\n\n\n# Create a quick assesment of sequences lost throughout the process\npng(\"results/plots/03.preview_reads_passing_ASV_processing.png\")\n# And same thing for the percentage remaining\nmatplot(t(track[,-5]/track[,1]*100),type='l',xaxt='n', main=\"Sequences remaining after each step  - R1 (%)\", xlab=\"Step\", ylab=\" Percentage of Sequences remaining\")\naxis(1,at=1:ncol(track[,-5]),labels=colnames(track[,-5]))\n# R2\nmatplot(t(track[,-4]/track[,1]*100),type='l',xaxt='n', main=\"Sequences remaining after each step  - R2 (%)\", xlab=\"Step\", ylab=\" Percentage of Sequences remaining\")\naxis(1,at=1:ncol(track[,-4]),labels=colnames(track[,-4]))\ndev.off()\n\n##Add final table\ntrack2 <- data.frame(track)\ntrack2$percentage_used <-(track2$nonchim / track2$Raw) * 100\ntrack2\nwrite.table(track2, \"results/03.Dada2/Seqs_lost_in_ASVs_processing_percentage.tsv\", col.names=NA, sep=\"\\t\")\n\n# Save work so far\nsave.image(file = \"src/Dada2.RData\") \n```\n\n## 03. Import to QIIME2\n\n\n\n\n```{bash}\n#!/usr/bin/bash\n## DianaOaxaca\n## Import data to QIIME2\n\n#Run in qiime conda environment\n#conda activate qiime2-2023.5\n\n#import rep seqs\nqiime tools import --input-path results/03.Dada2/ASVs.fasta --type 'FeatureData[Sequence]' --output-path results/04.qiime/ASV_rep_seq.qza\n\n# append missing header to the table for import\ncat <(echo -n \"#OTU Table\") results/03.Dada2/ASV_to_seqs-nochim.tsv > temp.txt\n\n# convert to biom\nbiom convert -i temp.txt -o temp.biom --table-type=\"OTU table\" --to-hdf5\n\n# and create table-type qza\nqiime tools import --input-path temp.biom --type 'FeatureTable[Frequency]' --input-format BIOMV210Format --output-path results/04.qiime/ASV_table.qza\n\n# remove temporal files\nrm temp.*\n```\n\n\n\n\n## 04. Taxonomic assignment\n\n\n\n\n```{bash}\n#!/usr/bin/bash\n## DianaOaxaca\n## Import data to QIIME2\n\n#Run in qiime conda environment\n#conda activate qiime2-2023.5\n\n#import rep seqs\nqiime tools import --input-path results/03.Dada2/ASVs.fasta --type 'FeatureData[Sequence]' --output-path results/04.qiime/ASV_rep_seq.qza\n\n# append missing header to the table for import\ncat <(echo -n \"#OTU Table\") results/03.Dada2/ASV_to_seqs-nochim.tsv > temp.txt\n\n# convert to biom\nbiom convert -i temp.txt -o temp.biom --table-type=\"OTU table\" --to-hdf5\n\n# and create table-type qza\nqiime tools import --input-path temp.biom --type 'FeatureTable[Frequency]' --input-format BIOMV210Format --output-path results/04.qiime/ASV_table.qza\n\n# remove temporal files\nrm temp.*\n```\n\n\n\n\n## 05. Filters\n\n\n\n\n```{bash}\n#| eval: false\n#| include: false\n#!/usr/bin/bash\n#DianaOaxaca\n#Filters\n\n#Summary of the qza table imported from R\nqiime feature-table summarize \\\n--i-table results/04.qiime/ASV_table.qza \\\n--o-visualization results/04.qiime/ASV_table.qzv\n\n#Filter by frequency\n#Here I removed all ASVs with a frequency of less than 0.1% of the mean sample depth. \n#This cut-off excludes ASVs that are likely due to MiSeq bleed-through between runs (reported by Illumina to be 0.1% of reads). \n#To calculate this cut-off I identified the mean sample depth, multiplied it by 0.001, and rounded to the nearest integer. \n#This step are describe in [this paper](https://journals.asm.org/doi/pdf/10.1128/msystems.00127-16)\n\nqiime feature-table filter-features --i-table  results/04.qiime/ASV_table.qza \\\n --p-min-samples 1 --p-min-frequency 218 --o-filtered-table results/04.qiime/ASV_table_filter_freq218.qza\n\nqiime feature-table summarize --i-table results/04.qiime/ASV_table_filter_freq218.qza \\\n --o-visualization results/04.qiime/ASV_table_filter_freq218.qzv\n\n#Filter Mitochondria, chloroplast and Eukaryota\n\nqiime taxa filter-table --i-table results/04.qiime/ASV_table_filter_freq218.qza \\\n --i-taxonomy results/04.qiime/taxonomy.qza --p-exclude Eukaryota,Mitochondria,Chloroplast \\\n --p-include p__ --o-filtered-table results/04.qiime/ASV_table_filter_freq218_emc.qza\n\nqiime feature-table summarize --i-table results/04.qiime/ASV_table_filter_freq218_emc.qza \\\n --o-visualization results/04.qiime/ASV_table_filter_freq218_emc.qzv\n\n#remove in fasta sequences\nqiime feature-table filter-seqs  --i-table results/04.qiime/ASV_table_filter_freq218_emc.qza \\\n --i-data results/04.qiime/ASV_rep_seq.qza --o-filtered-data results/04.qiime/ASV_rep_seq_filters.qza\n```\n\n\n\n\n## 06. Phylogeny\n\n\n\n\n```{bash}\n#!/usr/bin/bash\n#Get iqtree phylogeny\n\ndate\necho \"Start phylogeny\"\n\nqiime phylogeny align-to-tree-mafft-iqtree \\\n --p-n-threads auto --i-sequences results/04.qiime/ASV_rep_seq_filters.qza \\\n --o-alignment results/04.qiime/align.qza \\\n --o-masked-alignment results/04.qiime/masked-align.qza \\\n --o-tree results/04.qiime/unrooted-tree-iqtree.qza \\\n --o-rooted-tree results/04.qiime/rooted-tree-iqtree.qza --verbose\n\necho \"finish phylogeny!\"\n\ndate\n```\n\n",
    "supporting": [
      "01.Preprocessing_files"
    ],
    "filters": [],
    "includes": {}
  }
}